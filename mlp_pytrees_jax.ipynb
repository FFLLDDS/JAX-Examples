{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple MLP with JAX (using pytree-functionality for parameters)\n",
    "\n",
    "(see JAX docs, e.g. Tutorial: JAX 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit\n",
    "from jax import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'data'\n",
    "BATCH_SIZE = 64\n",
    "MNIST_IMG_SIZE = (28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import MNIST Dataset with Pytorch...\n",
    "\n",
    "... and set up Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def custom_transform(x):\n",
    "    \"\"\" gets PIL Image and returns flattened and normalized ndarray \"\"\"\n",
    "        \n",
    "    return np.ravel(np.array(x, dtype=np.float32))/255.0\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\" gets list of tuples and returns seperated images and labels as ndarrays \"\"\"\n",
    "    \n",
    "    transposed_data = list(zip(*batch))\n",
    "\n",
    "    labels = np.array(transposed_data[1])\n",
    "    imgs = np.stack(transposed_data[0])\n",
    "\n",
    "    return  imgs, labels\n",
    "\n",
    "\n",
    "train_dataset = MNIST(root=PATH, train=True, download=True, transform=custom_transform)\n",
    "test_dataset = MNIST(root=PATH, train=False, download=True , transform=custom_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# img = np.expand_dims(np.reshape(next(iter(train_loader))[0][0], (28,28)), axis=2) \n",
    "img = np.reshape(next(iter(train_loader))[0][0], MNIST_IMG_SIZE)\n",
    "\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initialize parameters of MLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_mlp_params(layer_widths, key):\n",
    "  params = []\n",
    "  for n_in, n_out in zip(layer_widths[:-1], layer_widths[1:]):\n",
    "    key, subkey = random.split(key)\n",
    "    params.append(\n",
    "        dict(weights=random.normal(subkey, shape=(n_in, n_out)) * np.sqrt(2/n_in),\n",
    "             biases=np.ones(shape=(n_out,))\n",
    "            )\n",
    "    )\n",
    "  return params\n",
    "\n",
    "# random key for parameter-initialization\n",
    "key = random.PRNGKey(42)\n",
    "\n",
    "params = init_mlp_params([28*28, 28*28, 512,  10], key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### use JAX pytree-functionality to check parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.tree_map(lambda x: x.shape, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define forward path, loss including forward path & parameter update function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(params, x):\n",
    "  *hidden, last = params\n",
    "  for layer in hidden:\n",
    "    x = jax.nn.relu(x @ layer['weights'] + layer['biases'])\n",
    "  return x @ last['weights'] + last['biases']\n",
    "\n",
    "batch_forward = jax.vmap(forward, in_axes=[None, 0]) \n",
    "\n",
    "def loss_fn(params, x, y):\n",
    "  return jnp.mean((batch_forward(params, x) - y) ** 2)\n",
    "\n",
    "def predict(params, x): \n",
    "  y_hat = batch_forward(params, x)\n",
    "  return_hat = np.argmax(batch_forward(params, x))\n",
    "  return np.argmax(batch_forward(params, x))\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "@jit\n",
    "def update(params, x, y):\n",
    "\n",
    "  grads = jax.grad(loss_fn)(params, x, y)\n",
    "  # Note that `grads` is a pytree with the same structure as `params`.\n",
    "  # `jax.grad` is one of the many JAX functions that has\n",
    "  # built-in support for pytrees.\n",
    "\n",
    "  # This is handy, because we can apply the SGD update using tree utils:\n",
    "  return jax.tree_multimap(lambda p, g: p - LEARNING_RATE * g, params, grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(x, k=10, dtype=jnp.float32):\n",
    "  \"\"\" create one-hot encodings of size k of (j)np.array x \"\"\"\n",
    "  return jnp.array(x[:, None] == jnp.arange(k), dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5\n",
    "\n",
    "for epoch in range(NUM_EPOCHS): \n",
    "    \n",
    "  for xs, ys in train_loader:\n",
    "    ys_onehot = one_hot(ys)\n",
    "    params = update(params, xs, ys_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f_imgs, labels = next(iter(train_loader))\n",
    "\n",
    "img_flat = f_imgs[0]\n",
    "label = labels[0]\n",
    "\n",
    "print(f'predicted: {predict(params, np.expand_dims(img_flat, axis=0))}, label: {label}')\n",
    "\n",
    "img = np.reshape(img_flat, MNIST_IMG_SIZE)\n",
    "plt.imshow(img)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8e38f0c188ce6a35b54fcffb420782e7c865c9fec8ce80c6a870b6cb8f84f9de"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('island')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
