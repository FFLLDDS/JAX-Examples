{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple MLP MNIST Classification with JAX\n",
    "\n",
    "(see JAX docs, e.g. ADVANCED JAX TUTORIALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit\n",
    "from jax import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'data'\n",
    "BATCH_SIZE = 64\n",
    "MNIST_IMG_SIZE = (28, 28, 1)\n",
    "STEP_SIZE = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import MNIST Dataset with Pytorch...\n",
    "\n",
    "... and set up Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def custom_transform(x):\n",
    "    \"\"\" gets PIL Image and returns flattened and normalized ndarray \"\"\"\n",
    "        \n",
    "    return np.ravel(np.array(x, dtype=np.float32))/255.0\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\" gets list of tuples and returns seperated images and labels as ndarrays \"\"\"\n",
    "    \n",
    "    transposed_data = list(zip(*batch))\n",
    "\n",
    "    labels = np.array(transposed_data[1])\n",
    "    imgs = np.stack(transposed_data[0])\n",
    "\n",
    "    return  imgs, labels\n",
    "\n",
    "\n",
    "train_dataset = MNIST(root=PATH, train=True, download=True, transform=custom_transform)\n",
    "test_dataset = MNIST(root=PATH, train=False, download=True , transform=custom_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# img = np.expand_dims(np.reshape(next(iter(train_loader))[0][0], (28,28)), axis=2) \n",
    "img = np.reshape(next(iter(train_loader))[0][0], MNIST_IMG_SIZE)\n",
    "\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initialize parameters of MLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to randomly initialize weights and biases\n",
    "# for a dense neural network layer\n",
    "def random_layer_params(m, n, key, scale=1e-2):\n",
    "  w_key, b_key = random.split(key)\n",
    "  return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))\n",
    "\n",
    "# # better initialization: (better predictions)\n",
    "# def random_layer_params(m, n, key):\n",
    "#  return np.sqrt(2/m) * random.normal(key, (n, m)), jnp.ones(shape=(n,))\n",
    "\n",
    "\n",
    "# Initialize all layers for a fully-connected neural network with sizes \"sizes\"\n",
    "def init_network_params(sizes, key):\n",
    "  keys = random.split(key, len(sizes))\n",
    "  return [random_layer_params(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n",
    "\n",
    "LAYER_SIZES = [784, 512, 512, 10]\n",
    "\n",
    "params = init_network_params(LAYER_SIZES, random.PRNGKey(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### use JAX pytree-functionality to check parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.tree_map(lambda x: x.shape, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define predict/forward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.scipy.special import logsumexp\n",
    "\n",
    "def relu(x):\n",
    "  return jnp.maximum(0, x)\n",
    "\n",
    "def predict(params, image):\n",
    "  # per-example predictions\n",
    "  activations = image\n",
    "  for w, b in params[:-1]:\n",
    "    outputs = jnp.dot(w, activations) + b\n",
    "    activations = relu(outputs)\n",
    "  \n",
    "  final_w, final_b = params[-1]\n",
    "  logits = jnp.dot(final_w, activations) + final_b\n",
    "  return logits\n",
    "\n",
    "# vmap predict to handle batches\n",
    "batch_predict = jax.vmap(predict, in_axes=(None, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define loss and update function\n",
    "\n",
    "The loss function here is I think a version of the negative log-likelihood function, a.k.a. Cross Entropy. If one defines the Likelihood in the following way, with $ \\sigma $ being the Softmax, \n",
    "\n",
    "$$ L(\\theta) = p_{(Y|X)}(y|x) = \\prod_{i=1}^{n} \\prod_{k=1}^{l} \\sigma(Net(x_i))_k^{y_{i_k}} $$\n",
    "\n",
    "Applying the logarithm and multiplying by minus one we get the Negative Log Likelihood: \n",
    "\n",
    "$$  - log(L(\\theta)) = - \\sum_{i=1}^{n} \\sum_{k=1}^{l} y_{i_k} (Net(x_i)_k - log( \\sum_{j=1}^l e^{Net(x_i)_j} )) $$\n",
    "\n",
    "<font size=\"2\">(And deviding by the number of samples gives the Cross Entropy).</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TARGETS = 10\n",
    "\n",
    "def one_hot(x, k=N_TARGETS, dtype=jnp.float32):\n",
    "  \"\"\" create one-hot encodings of size k of (j)np.array x \"\"\"\n",
    "  return jnp.array(x[:, None] == jnp.arange(k), dtype)\n",
    "  \n",
    "def accuracy(params, images, targets):\n",
    "  predicted_class = jnp.argmax(batch_predict(params, images), axis=1)\n",
    "  return jnp.mean(predicted_class == targets)\n",
    "\n",
    "def loss(params, images, targets):\n",
    "  logits = batch_predict(params, images)\n",
    "  return -jnp.mean((logits- logsumexp(logits)) * targets)\n",
    "\n",
    "@jit\n",
    "def update(params, x, y):\n",
    "  grads = jax.grad(loss)(params, x, y)\n",
    "  return [(w - STEP_SIZE * dw, b - STEP_SIZE * db)\n",
    "          for (w, b), (dw, db) in zip(params, grads)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 3\n",
    "\n",
    "for epoch in range(NUM_EPOCHS): \n",
    "    \n",
    "  for xs, ys in train_loader:\n",
    "    ys_onehot = one_hot(ys)\n",
    "    params = update(params, xs, ys_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f_imgs, labels = next(iter(test_loader))\n",
    "\n",
    "r_int = np.random.randint(0, BATCH_SIZE-1)\n",
    "img_flat = f_imgs[r_int]\n",
    "label = labels[r_int]\n",
    "\n",
    "out = batch_predict(params, np.expand_dims(img_flat, axis=0))\n",
    "prediction = jnp.argmax(out)\n",
    "\n",
    "print('accuracy on sample batch: ', accuracy(params, f_imgs, labels))\n",
    "print(f'example \\n predicted: {prediction}, label: {label}')\n",
    "\n",
    "img = np.reshape(img_flat, MNIST_IMG_SIZE)\n",
    "plt.imshow(img)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8e38f0c188ce6a35b54fcffb420782e7c865c9fec8ce80c6a870b6cb8f84f9de"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('island')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
